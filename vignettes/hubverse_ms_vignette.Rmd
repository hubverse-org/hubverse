---
title: "Supplemental Code Demo"
subtitle: "Accessing Data from the US Flusight Hubverse Project"
author: 
- Consortium of Infectious Disease Modeling Hubs
- Melissa Kerr
- Rebecca Borchering
- Alvaro Castro Rivadeneira
- Lucie Contamin
- Sebastian Funk
- Harry Hochheiser
- Emily Howerton
- Anna Krystalli
- Li Shandross
- Nicholas G Reich
date: "`r Sys.Date()`"
output: 
  pdf_document:
    citation_package: natbib
bibliography: references.bib
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE
)
```

# Required software

The following demonstration shows how to access and analyze data from a real hub using `r R.version$string` and several of the hubverse R packages.[@the_consortium_of_infectious_disease_modeling_hubs_hubverse_2025]

The hubverse packages `hubData`, `hubVis`, `hubEnsembles` and `hubEvals` are best installed through the R-universe package repository to ensure you are working with the latest released versions.

```{r install, eval=FALSE}
install.packages(
  pkgs = c("hubData", "hubVis", "hubEnsembles", "hubEvals"),
  repos = c(
    "https://hubverse-org.r-universe.dev",
    "https://cloud.r-project.org"
  )
)
```

Supporting packages `ggplot2` and `dplyr` can be installed from CRAN.

```{r install-CRAN, eval=FALSE}
install.packages(
  pkgs = c("dplyr", "ggplot2")
)
```

Once the required packages are installed, load the supporting packages into your R session. (Session information on all the packages used including package versions are included at the end of this vignette.)

```{r libraries, message=FALSE}
# Data munging
library(dplyr)
# Plotting
library(ggplot2)
```


# Load forecast data

CDC's FluSight Forecast Hub exists as a [GitHub repository](https://github.com/cdcepi/FluSight-forecast-hub).[@noauthor_cdcepiflusight-forecast-hub_2025] Using hubverse tools, it also has been mirrored to the cloud, so a copy of the data lives at a publicly-accessible AWS S3 bucket. This means that instead of needing to download a local copy of the repository, the data for this project can be directly accessed from the cloud using `hubData` tools.

We start by initializing a connection to the hub in the cloud. Printing displays metadata about the resulting connection.

```{r}
hub_path <- hubData::s3_bucket("cdcepi-flusight-forecast-hub/")

hub_con <- hubData::connect_hub(hub_path, skip_checks = TRUE)
hub_con
```

We then collect a subset of model outputs that we will interact with further in this demonstration. Specifically, we will extract zero through three week-ahead quantile predictions of weekly incident flu hospitalizations made for the state of Texas on three dates in late 2023 and early 2024. For simplicity, we will restrict our query to predictions made for five arbitrarily chosen models, although predictions for over 30 models are available on each of these dates.

```{r}
models <- c(
  "FluSight-baseline",
  "CMU-TimeSeries",
  "UMass-flusion",
  "cfa-flumech",
  "UGA_flucast-INFLAenza"
)
reference_dates <- c("2023-12-16", "2024-01-13", "2024-02-10")

flusight_data <- hub_con |>
  filter(
    target == "wk inc flu hosp",
    reference_date %in% reference_dates,
    model_id %in% models,
    output_type == "quantile",
    location == "48", ## FIPS code for Texas
    horizon > -1
  ) |>
  hubData::collect_hub()
```


```{r, echo=FALSE}
knitr::kable(
  flusight_data[
    116:120,
    c(
      "model_id", "reference_date", "location",
      "output_type", "output_type_id", "value"
    )
  ],
  digits = 2,
  caption = "Five rows of the `flusight_data` object collected above.
             The \"target\", \"horizon\", and \"target_end_date\" columns have
             been excluded for readability. The \"output_type_id\" and \"value\"
             columns show the different predicted quantiles for the number of
             hospital admissions due to influenza made by the CMU-TimeSeries
             model for location 48 (Texas). The predictions were made during the
             week of December 16, 2023 (reference date) and are making a
             prediction for the horizon of 1 week ahead, for the week ending
             December 23."
)
```


# Loading target data

To be able to visualise forecasts in relation to observed data, we need to load the target (observed) data for the locations and time period we are interested in. 

Hubs generally contain two types of target data: time-series data and oracle output data. Time-series data is stored in a long format, with one row per observation, a single observed value per observation unit and is the format used for visualisation of observed target data. Oracle output data on the other hand is often derived from time-series data but more closely matches the model output data format and is used for the evaluation of model outputs. The FluSight hub stores target data in both formats, but we will first load the time-series format to visualise the performance of hub models against observed data. For more information on target data formats, see the [hubverse documentation](https://docs.hubverse.io/en/latest/user-guide/target-data.html).

Accessing target data follows a similar pattern to accessing model output data. We start by connecting to the target data type of interest in the hub. To access the time-series format we use `hubData::connect_target_timeseries()` function.

```{r}
ts_data_con <- hubData::connect_target_timeseries(hub_path)
```

Next, we filter the target data to the location and time period of interest. In this case, we are interested in the state of Texas (FIPS code 48) and the 2023-2024 respiratory virus season, which runs from September 23, 2023 to May 1, 2024. We also filter to the most recent `as_of` date to ensure we are using the latest available data. The `as_of` date is a metadata field that indicates when the target data was last updated.

To enable filtering by the maximum (latest) value of `as_of`, we use the `arrow::to_duckdb()` function to convert the connection to a memory virtual DuckDB table. This allows us to run queries that are supported by DuckDB but not by arrow, including filtering for the maximum value of a column. We then filter the data to the desired location, time period, and maximum `as_of` date, and select the relevant columns. Finally, we collect the data into a local data frame and rename the `target_end_date` column to `date` which is what the `hubVis` package functions expect.

```{r}
target_data <- ts_data_con |>
  arrow::to_duckdb() |>
  filter(
    location == "48",
    target_end_date >= "2023-09-23",
    target_end_date <= "2024-05-01",
    as_of == max(as_of)
  ) |>
  select(location, location_name, target, target_end_date, observation) |>
  arrange(target_end_date) |>
  collect() |>
  rename(date = target_end_date)
```

The resulting target data contains the actual number of hospital admissions due to influenza in Texas for the 2023-2024 respiratory virus season, which is the same target that hub models the models are predicting.

```{r, echo=FALSE}
knitr::kable(target_data[1:5, ],
  digits = 2,
  caption = "Five rows of the `target_data` object read in above.
             The observation column shows the number of hospital admissions
             due to influenza reported in Texas (location 48) on the week
             ending on each date."
)
```

\clearpage 

## Visualize forecasts

We can use the hubVis package to create a single graphic that overlays the forecasts from the five models and three forecast dates with the observed data.

First we set the default theme for plots to be a clean black and white theme.

```{r}
theme_set(theme_bw())
```

Next we use function `plot_step_ahead_model_output()` to visualize the model output data in relation to target data. This function takes the model output data and target data as input, and plots the predictions from each model along with the observed data. The `x_col_name` argument specifies the column to use for the x-axis, which is `target_end_date` in this case. The `use_median_as_point` argument specifies whether to use the median prediction as the point estimate for each model, and `group` specifies how to group the data for plotting. The `fill_transparency` argument controls the transparency of the shaded prediction intervals, and `intervals` specifies the width of the prediction intervals to plot.

```{r, fig.cap = "Forecasts of hospital admissions in Texas due to influenza in the 2023-2024 respiratory virus season. Predictions from five models are shown along with observed data (line with dots for specific observations). Each model is shown in a different color, with the 80% prediction intervals shown as a shaded region.", fig.width = 9, fig.height = 6}  
flusight_data |>
  hubVis::plot_step_ahead_model_output(
    target_data,
    x_col_name = "target_end_date",
    use_median_as_point = TRUE,
    group = "reference_date",
    interactive = FALSE,
    fill_transparency = .1,
    intervals = 0.8
  ) +
  theme(legend.position = "bottom") +
  ylab("hospital admissions due to influenza")
```

\clearpage 

# Build ensemble and visualize

Using the model output data collected above, we can pass these data to the `hubEnsembles::simple_ensemble()` function to build an ensemble that takes the mean of predicted values from each model at each quantile level. This new forecast output is then added to the existing forecasts and visualized all together. The hubEnsembles package supports a range of aggregation approaches including a linear opinion pool (probability density averaging) or using the mean or median of quantiles.[@shandross_multi-model_2025] Model weights can be specified for the various approaches, allowing for some models to influence the output more strongly.

```{r, fig.cap = "Forecasts of hospital admissions in Texas due to influenza in the 2023-2024 respiratory virus season. Predictions from five submitted models and an ensemble are shown along with observed data. The observed data are shown as a line with dots for specific weekly observations. Each individual model is shown in orange and the mean ensemble is shown in black, with the 80% prediction intervals shown as shaded regions."}
ensemble_forecast <- hubEnsembles::simple_ensemble(flusight_data)

all_forecasts <- flusight_data |>
  bind_rows(ensemble_forecast)

all_forecasts |>
  hubVis::plot_step_ahead_model_output(target_data,
    x_col_name = "target_end_date",
    use_median_as_point = TRUE,
    group = "reference_date",
    interactive = FALSE,
    fill_transparency = .2,
    intervals = 0.90,
    one_color = "darkgray",
    ens_name = "hub-ensemble",
    ens_color = "orange",
    pal_color = NULL
  ) +
  theme(legend.position = "bottom") +
  ylab("hospital admissions due to influenza")
```

\clearpage

# Evaluate forecasts

The package hubEvals can be used to evaluate probabilistic predictions against observed data. The hubEvals package creates an interface for hubverse-style model output and target data to the scoringutils package, which is an engine for computing a number of common scores for probabilistic predictions.[@bosse-scoringutils-2024] 

The code below starts by making oracle output data, which is a dataset formatted like hubverse-style model output data but with a point prediction representing the eventual observation, as if observed by an oracle model. The FluSight hub stores target data in a time-series format but not in an oracle output format. The oracle output data format is convenient to use for scoring, as it can be joined with model output data and then the observation and the prediction reside in a single row of stored data. In this example, we compute and show results for the weighted interval score (WIS) with its associated decomposition into overprediction, underprediction, and dispersion penalties, the 50% and 90% prediction interval coverage rates, and the absolute error computed on the median prediction.[@bracher_evaluating_2021]

```{r, fig.cap = "Average weighted interval score (WIS) computed by model (y-axis). The total length of each bar is the average WIS across all predictions made. WIS can be decomposed into penalties for overprediction, underprediction and dispersion, the contributions of which are shown by the segments of the bar. The UMass-flusion model has the lowest (best) average WIS, and it received a greater penalty for underprediction relative to the penalty for overprediction."}
## make oracle output
oracle_output <- target_data |>
  dplyr::select(date, location, observation) |>
  dplyr::rename(target_end_date = date,
                oracle_value = observation) |>
  tidyr::crossing(horizon = 0:3) |>
  dplyr::mutate(reference_date = as.Date(target_end_date) - horizon * 7L) |>
  dplyr::filter(reference_date %in% reference_dates)

scores <- hubEvals::score_model_out(all_forecasts,
                                    oracle_output,
                                    baseline = "FluSight-baseline") |>
  dplyr::arrange(wis)

scoringutils::plot_wis(scores, x = "model_id")
```


```{r, echo=FALSE}
knitr::kable(scores[, c(1:2, 7:9)], digits = 2,
             caption = "A print-out of the `scores` object computed above,
             containing the summary metrics for the five component models and
             the ensemble model. Columns including other metrics, including
             bias, underprediction, overprediction and dispersion were omitted
             for space.")
```

\clearpage
# Session information

```{r, load-libraries-for-session-info, include=FALSE}
library(hubData)
library(hubEnsembles)
library(hubVis)
library(hubEvals)
```


```{r}
sessionInfo()
```

